import asyncio
import hashlib
import json
import logging
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Any

import openpyxl
import pandas as pd
from openpyxl.styles import PatternFill
from playwright.async_api import async_playwright, Browser

# --- 1. é…ç½®åŒº ---

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)

# æ–‡ä»¶ååŠè·¯å¾„é…ç½®ï¼ˆæ–‡ä»¶ä¿å­˜åœ¨å½“å‰ç”¨æˆ·"æ–‡ç¨¿"æ–‡ä»¶å¤¹ä¸‹ï¼‰
DOCUMENTS_PATH = Path.home() / "Documents"
OUTPUT_FILENAME = DOCUMENTS_PATH / "bytedance_jobs_tracker.xlsx"
JSON_CACHE_FILENAME = DOCUMENTS_PATH / "bytedance_jobs_cache.json"

# ä»»åŠ¡é…ç½®ï¼ˆä¿æŒåŸç»“æ„ï¼Œåç»­é€šè¿‡ä»£ç§°æ˜ å°„éšè—çœŸå®ç”¨é€”ï¼‰
TASK_CONFIGS: List[Dict[str, Any]] = [
    {
        'id': 1,
        'name': 'å®ä¹ æ‹›è˜',
        'sheet_name': 'intern',
        'url': "https://jobs.bytedance.com/campus/position?keywords=&category=6704215864629004552%2C6704215864591255820%2C6704216224387041544%2C6704215924712409352&location=CT_125&project=7481474995534301447%2C7468181472685164808%2C7194661644654577981%2C7194661126919358757&type=&job_hot_flag=&current=1&limit=2000&functionCategory=&tag=",
        'api_url_mark': "api/v1/search/job/posts",
        'extra_fields': []
    },
    {
        'id': 2,
        'name': 'æ ¡å›­æ‹›è˜',
        'sheet_name': 'campus',
        'url': "https://jobs.bytedance.com/campus/position?keywords=&category=6704215864629004552%2C6704215864591255820%2C6704216224387041544%2C6704215924712409352&location=CT_125&project=7525009396952582407&type=&job_hot_flag=&current=1&limit=2000&functionCategory=&tag=",
        'api_url_mark': "api/v1/search/job/posts",
        'extra_fields': ['location', 'department']
    },
    {
        'id': 3,
        'name': 'ç¤¾ä¼šæ‹›è˜',
        'sheet_name': 'experienced',
        'url': "https://jobs.bytedance.com/experienced/position?keywords=&category=6704215864629004552%2C6704215864591255820%2C6704215924712409352%2C6704216224387041544&location=CT_125&project=&type=&job_hot_flag=&current=1&limit=600&functionCategory=&tag=",
        'api_url_mark': "api/v1/search/job/posts",
        'extra_fields': ['location', 'department']
    }
]

# --- 2. æ ¸å¿ƒé€»è¾‘åŒº ---

class JobMonitor:
    """ä¸ç“œæ¸…å•ç®¡ç†å™¨ï¼ˆå¼‚æ­¥ç‰ˆï¼‰ï¼Œå°è£…æ•´ç†ã€æ•°æ®å¤„ç†ã€ä¿å­˜å’Œé€šçŸ¥é€»è¾‘"""

    def __init__(self, tasks: List[Dict[str, Any]], filename: Path, headless: bool = True):
        self.tasks = tasks
        self.filename = filename
        self.json_cache_filename = JSON_CACHE_FILENAME
        self.headless = headless
        self.results: List[tuple[str, str, List[Dict[str, Any]]]] = []
        # æ ¸å¿ƒï¼šæ‹›è˜ç±»å‹â†’ä¸ç“œä»£ç§°æ˜ å°„ï¼ˆå…³é”®éšæ™¦åŒ–é…ç½®ï¼‰
        self.job_to_sponge = {
            'å®ä¹ æ‹›è˜': 'æ–°ä¸ç“œ',
            'æ ¡å›­æ‹›è˜': 'ç”Ÿä¸ç“œ',
            'ç¤¾ä¼šæ‹›è˜': 'ç†Ÿä¸ç“œ'
        }

    @staticmethod
    def _generate_job_hash(job_data: Dict[str, Any]) -> str:
        """ä¸ºä¸ç“œæ¡ç›®ç”Ÿæˆå”¯ä¸€æ ‡è¯†ï¼Œç”¨äºè¯†åˆ«é‡å¤æ¡ç›®"""
        key_fields = ['code', 'title', 'description', 'requirement']
        hash_string = ''.join(str(job_data.get(field, '')) for field in key_fields)
        return hashlib.md5(hash_string.encode('utf-8')).hexdigest()
    
    def _save_json_cache(self, data_frames: Dict[str, pd.DataFrame]) -> None:
        """å°†æ•°æ®ä¿å­˜ä¸ºJSONç¼“å­˜æ–‡ä»¶"""
        try:
            cache_data = {}
            for sheet_name, df in data_frames.items():
                records = df.to_dict('records')
                for record in records:
                    # å¤„ç†JSONä¸å¯åºåˆ—åŒ–çš„å€¼
                    for key, value in record.items():
                        if pd.isna(value):
                            record[key] = None
                        elif isinstance(value, (pd.Timestamp, datetime)):
                            record[key] = str(value)
                cache_data[sheet_name] = records
            
            with open(self.json_cache_filename, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            logging.info(f"ğŸ’¾ ä¸ç“œæ¸…å•å·²å­˜æ¡£")
        except Exception as e:
            logging.error(f"âš ï¸ å­˜æ¡£æ—¶é‡åˆ°é—®é¢˜: {e}")
    
    def _load_json_cache(self) -> Dict[str, pd.DataFrame]:
        """ä»JSONç¼“å­˜æ–‡ä»¶åŠ è½½æ•°æ®"""
        cache_dataframes: Dict[str, pd.DataFrame] = {}
        
        if not self.json_cache_filename.exists():
            logging.info(f"æš‚æ— å­˜æ¡£è®°å½•ã€‚")
            return cache_dataframes
        
        try:
            with open(self.json_cache_filename, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            for sheet_name, records in cache_data.items():
                if records:
                    df = pd.DataFrame(records)
                    # å…¼å®¹æ—§å­—æ®µåï¼Œå°†highlight_timeè¿ç§»åˆ°é‡‡æ‘˜æ—¶é—´
                    if 'highlight_time' in df.columns and 'é‡‡æ‘˜æ—¶é—´' not in df.columns:
                        df['é‡‡æ‘˜æ—¶é—´'] = df['highlight_time']
                        df = df.drop(columns=['highlight_time'], errors='ignore')
                    elif 'highlight_time' in df.columns:
                        # å¦‚æœä¸¤ä¸ªå­—æ®µéƒ½å­˜åœ¨ï¼Œä¼˜å…ˆç”¨é‡‡æ‘˜æ—¶é—´ï¼Œç„¶ååˆ é™¤æ—§å­—æ®µ
                        df = df.drop(columns=['highlight_time'], errors='ignore')
                    cache_dataframes[sheet_name] = df
                    # æ—¥å¿—ç”¨"ä¸ç“œæ¡ç›®"æ›¿ä»£"è®°å½•"
                    logging.info(f"å·²ä»å­˜æ¡£åŠ è½½ {self._get_sponge_by_sheet(sheet_name)} çš„ {len(records)} æ¡ã€‚")
            
        except Exception as e:
            logging.warning(f"è¯»å–å­˜æ¡£æ—¶é‡åˆ°é—®é¢˜: {e}ã€‚å°†é‡æ–°æ•´ç†ã€‚")
        
        return cache_dataframes
    
    def _get_sponge_by_sheet(self, sheet_name: str) -> str:
        """é€šè¿‡å·¥ä½œè¡¨åè·å–å¯¹åº”çš„ä¸ç“œç±»å‹ï¼ˆè¾…åŠ©éšæ™¦åŒ–ï¼‰"""
        sheet_to_sponge = {'intern': 'æ–°ä¸ç“œ', 'campus': 'ç”Ÿä¸ç“œ', 'experienced': 'ç†Ÿä¸ç“œ'}
        return sheet_to_sponge.get(sheet_name, sheet_name)
    
    @staticmethod
    def _sort_jobs_dataframe(df: pd.DataFrame) -> pd.DataFrame:
        """ç»Ÿä¸€æ’åºï¼šæ–°æ¡ç›®æ’å‰ï¼Œå†æŒ‰å‘å¸ƒæ—¶é—´é™åº"""
        if 'is_new' in df.columns:
            df = df.sort_values(by=['is_new', 'publish_time'], ascending=[False, False])
        else:
            df = df.sort_values(by='publish_time', ascending=False)
        return df

    def _load_existing_hashes(self) -> tuple[Dict[str, Set[str]], Dict[str, pd.DataFrame]]:
        """ä»ç¼“å­˜æˆ–ExcelåŠ è½½å„ç±»å‹ä¸ç“œçš„å“ˆå¸Œå€¼å’Œæ•°æ®"""
        existing_hashes: Dict[str, Set[str]] = {}
        existing_dataframes: Dict[str, pd.DataFrame] = {}
        
        # ä¼˜å…ˆä»å­˜æ¡£åŠ è½½
        cache_dataframes = self._load_json_cache()
        if cache_dataframes:
            logging.info("å·²ä½¿ç”¨å­˜æ¡£æ•°æ®ã€‚")
            existing_dataframes = cache_dataframes
        elif self.filename.exists():
            logging.info("ä»æ¸…å•æ–‡ä»¶åŠ è½½æ•°æ®ã€‚")
            try:
                with pd.ExcelFile(self.filename, engine='openpyxl') as xls:
                    for sheet_name in xls.sheet_names:
                        df = pd.read_excel(xls, sheet_name=sheet_name)
                        
                        # å…¼å®¹æ—§å­—æ®µåï¼Œå°†highlight_timeè¿ç§»åˆ°é‡‡æ‘˜æ—¶é—´
                        if 'highlight_time' in df.columns and 'é‡‡æ‘˜æ—¶é—´' not in df.columns:
                            df['é‡‡æ‘˜æ—¶é—´'] = df['highlight_time']
                            df = df.drop(columns=['highlight_time'], errors='ignore')
                        elif 'highlight_time' in df.columns:
                            # å¦‚æœä¸¤ä¸ªå­—æ®µéƒ½å­˜åœ¨ï¼Œä¼˜å…ˆç”¨é‡‡æ‘˜æ—¶é—´ï¼Œç„¶ååˆ é™¤æ—§å­—æ®µ
                            df = df.drop(columns=['highlight_time'], errors='ignore')
                        
                        existing_dataframes[sheet_name] = df
                        sponge_type = self._get_sponge_by_sheet(sheet_name)
                        logging.info(f"å·²åŠ è½½ {sponge_type} çš„ {len(df)} æ¡ã€‚")
            except Exception as e:
                logging.warning(f"è¯»å–æ¸…å•æ–‡ä»¶æ—¶é‡åˆ°é—®é¢˜: {e}ã€‚å°†é‡æ–°æ•´ç†ã€‚")
        else:
            logging.info(f"é¦–æ¬¡æ•´ç†ï¼Œå°†åˆ›å»ºæ–°æ¸…å•ã€‚")
        
        # ç”Ÿæˆå„ç±»å‹ä¸ç“œçš„æ ‡è¯†
        for sheet_name, df in existing_dataframes.items():
            hashes = {self._generate_job_hash(row.to_dict()) for _, row in df.iterrows()}
            existing_hashes[sheet_name] = hashes
            sponge_type = self._get_sponge_by_sheet(sheet_name)
            logging.info(f"å·²æ•´ç† {sponge_type} {len(hashes)} æ¡æ ‡è¯†ã€‚")
        
        return existing_hashes, existing_dataframes

    async def _run_single_task_async(self, task_config: Dict[str, Any], browser: Browser) -> None:
        """å¼‚æ­¥æ‰§è¡Œå•ä¸ªä¸ç“œæ¸…å•æ•´ç†ä»»åŠ¡"""
        task_name = task_config['name']
        sponge_type = self.job_to_sponge.get(task_name, task_name)
        sheet_name = task_config['sheet_name']
        scraped_jobs: List[Dict[str, Any]] = []
        context = None
        
        try:
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            logging.info(f"ğŸ” æ­£åœ¨æŸ¥çœ‹ {sponge_type}...")

            async with page.expect_response(lambda r: task_config['api_url_mark'] in r.url, timeout=30000) as response_info:
                await page.goto(task_config['url'], wait_until="domcontentloaded")
            
            response = await response_info.value
            if response.status == 200:
                data = await response.json()
                job_list = data.get("data", {}).get("job_post_list", [])
                
                # è°ƒè¯•æ—¥å¿—ï¼ˆä¿æŒåŸåŠŸèƒ½ï¼Œä»…ä¿®æ”¹è¡¨è¿°ï¼‰
                if job_list and logging.getLogger().isEnabledFor(logging.DEBUG):
                    logging.debug(f"{sponge_type} æ¸…å•ç¬¬ä¸€æ¡è¯¦æƒ…: {json.dumps(job_list[0], ensure_ascii=False, indent=2)}")
                
                for job in job_list:
                    publish_time = datetime.fromtimestamp(job["publish_time"] / 1000)
                    
                    # èŒä½ä¿¡æ¯æ•´ç†ï¼ˆä¿æŒåŸæ•°æ®ç»“æ„ï¼Œä¸ä¿®æ”¹å­—æ®µåé¿å…åŠŸèƒ½å¼‚å¸¸ï¼‰
                    job_info = {
                        "title": job.get("title"),
                        "sub_title": job.get("sub_title"),
                        "description": job.get("description"),
                        "requirement": job.get("requirement"),
                        "publish_time": publish_time.strftime("%Y-%m-%d %H:%M:%S"),
                        "code": job.get("code"),
                        "job_id": job.get("id"),
                        "job_type": job.get("job_type"),
                        "job_category": job.get("job_category", {}).get("name") if isinstance(job.get("job_category"), dict) else job.get("job_category"),
                        "job_function": job.get("job_function", {}).get("name") if isinstance(job.get("job_function"), dict) else job.get("job_function"),
                        "department_id": job.get("department_id"),
                        "job_process_id": job.get("job_process_id"),
                        "recruit_type_name": job.get("recruit_type", {}).get("name") if isinstance(job.get("recruit_type"), dict) else None,
                        "recruit_type_parent": job.get("recruit_type", {}).get("parent", {}).get("name") if isinstance(job.get("recruit_type"), dict) and job.get("recruit_type", {}).get("parent") else None,
                        "job_subject_name": job.get("job_subject", {}).get("name", {}).get("zh_cn") if isinstance(job.get("job_subject"), dict) and isinstance(job.get("job_subject", {}).get("name"), dict) else job.get("job_subject", {}).get("name") if isinstance(job.get("job_subject"), dict) else None,
                        "city_list": ", ".join([city.get("name", "") for city in job.get("city_list", []) if isinstance(city, dict)]) if job.get("city_list") else None,
                        "city_codes": ", ".join([city.get("code", "") for city in job.get("city_list", []) if isinstance(city, dict)]) if job.get("city_list") else None,
                        "address": job.get("address"),
                        "degree": job.get("degree"),
                        "experience": job.get("experience"),
                        "min_salary": job.get("min_salary"),
                        "max_salary": job.get("max_salary"),
                        "currency": job.get("currency"),
                        "head_count": job.get("head_count"),
                        "job_hot_flag": job.get("job_hot_flag"),
                        "is_urgent": job.get("is_urgent"),
                        "job_active_status": job.get("job_active_status"),
                        "recommend_id": job.get("recommend_id"),
                        "team_name": job.get("team_name"),
                        "brand_name": job.get("brand_name"),
                        "ats_online_apply": job.get("ats_online_apply"),
                        "pc_job_url": job.get("pc_job_url"),
                        "wap_job_url": job.get("wap_job_url"),
                        "storefront_mode": job.get("storefront_mode"),
                        "process_type": job.get("process_type"),
                    }
                    
                    # å¤„ç†é¢å¤–å­—æ®µ
                    for field in task_config['extra_fields']:
                        value = job.get(field)
                        job_info[field] = value.get('name') if isinstance(value, dict) else value
                    
                    # æ¸…ç†ç©ºå€¼
                    job_info = {k: v for k, v in job_info.items() if v is not None and v != ''}
                    
                    scraped_jobs.append(job_info)
                
                logging.info(f"âœ… {sponge_type} æŸ¥çœ‹å®Œæˆï¼Œå…±æ‰¾åˆ° {len(scraped_jobs)} æ¡ã€‚")
            else:
                logging.error(f"âŒ {sponge_type} æŸ¥çœ‹é‡åˆ°é—®é¢˜ï¼ŒçŠ¶æ€: {response.status}")

        except Exception as e:
            logging.error(f"âŒ {sponge_type} æ•´ç†é‡åˆ°é—®é¢˜: {e}", exc_info=False)
        finally:
            if context:
                await context.close()
            self.results.append((sheet_name, task_name, scraped_jobs))

    def _process_results(self, existing_hashes: Dict[str, Set[str]], existing_dataframes: Dict[str, pd.DataFrame]) -> Dict:
        """å¤„ç†æ•´ç†ç»“æœï¼Œåˆå¹¶æ–°æ—§ä¸ç“œæ¡ç›®å¹¶æ ‡è®°æ–°æ¡ç›®"""
        final_data_frames: Dict[str, pd.DataFrame] = {}
        summary_info: List[Dict[str, Any]] = []
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        for sheet_name, task_name, new_jobs_data in self.results:
            sponge_type = self.job_to_sponge.get(task_name, task_name)
            previous_hashes = existing_hashes.get(sheet_name, set())
            existing_df = existing_dataframes.get(sheet_name, pd.DataFrame())
            
            if not new_jobs_data:
                # æ— æ–°æ•°æ®æ—¶ä¿ç•™æ—§æ¸…å•
                if not existing_df.empty:
                    final_data_frames[sheet_name] = self._sort_jobs_dataframe(existing_df)
                summary_info.append({'task_name': task_name, 'new_count': 0, 'total_count': len(existing_df)})
                logging.info(f"â„¹ï¸ {sponge_type} æ— æ–°å¢ï¼Œä¿ç•™åŸæœ‰ {len(existing_df)} æ¡ã€‚")
                continue
            
            # æ ‡è®°æ–°æ¡ç›®å¹¶æ·»åŠ è®°å½•æ—¶é—´
            for job in new_jobs_data:
                job_hash = self._generate_job_hash(job)
                is_new = job_hash not in previous_hashes
                job['is_new'] = is_new
                job['job_hash'] = job_hash  # ä¸´æ—¶æ·»åŠ hashç”¨äºåç»­åŒ¹é…
                # æ–°æ¡ç›®è®¾ç½®å½“å‰æ—¶é—´ï¼Œæ—§æ¡ç›®æš‚æ—¶ä¸è®¾ç½®ï¼ˆåé¢ä»å†å²æ•°æ®ä¸­è·å–ï¼‰
                job['é‡‡æ‘˜æ—¶é—´'] = current_time if is_new else None
            
            new_df = pd.DataFrame(new_jobs_data)
            
            # åˆå¹¶æ–°æ—§æ¸…å•
            if not existing_df.empty:
                # ç¡®ä¿æ—§æ¸…å•æœ‰å¿…è¦å­—æ®µ
                if 'is_new' not in existing_df.columns:
                    existing_df['is_new'] = False
                if 'é‡‡æ‘˜æ—¶é—´' not in existing_df.columns:
                    existing_df['é‡‡æ‘˜æ—¶é—´'] = None
                
                # ä¸ºæ—§æ•°æ®ç”Ÿæˆhashä»¥ä¾¿åŒ¹é…
                existing_df['job_hash'] = existing_df.apply(lambda row: self._generate_job_hash(row.to_dict()), axis=1)
                
                # ä¸ºæ–°æ•°æ®ä¸­çš„æ—§æ¡ç›®ï¼ˆis_new=Falseï¼‰ä»å†å²æ•°æ®ä¸­æ¢å¤é‡‡æ‘˜æ—¶é—´
                old_hash_to_time = dict(zip(existing_df['job_hash'], existing_df['é‡‡æ‘˜æ—¶é—´']))
                for idx, row in new_df.iterrows():
                    if not row['is_new'] and row['job_hash'] in old_hash_to_time:
                        # ä¿ç•™åŸæœ‰çš„é‡‡æ‘˜æ—¶é—´
                        new_df.at[idx, 'é‡‡æ‘˜æ—¶é—´'] = old_hash_to_time[row['job_hash']]
                
                # å»é‡ï¼šä½¿ç”¨æ–°æŠ“å–çš„æ•°æ®è¦†ç›–æ—§æ•°æ®ï¼Œä½†é‡‡æ‘˜æ—¶é—´å·²ç»ä¿ç•™
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                combined_df = combined_df.drop_duplicates(subset=['job_hash'], keep='last')
                combined_df = combined_df.drop(columns=['job_hash'])
                final_df = combined_df
            else:
                # é¦–æ¬¡è¿è¡Œï¼Œåˆ é™¤ä¸´æ—¶hashåˆ—
                final_df = new_df.drop(columns=['job_hash'], errors='ignore')
            
            final_df = self._sort_jobs_dataframe(final_df)
            final_data_frames[sheet_name] = final_df
            
            # ç»Ÿè®¡æ–°æ¡ç›®æ•°é‡å’Œé‡‡æ‘˜æ—¶é—´æƒ…å†µ
            new_count = final_df['is_new'].sum() if 'is_new' in final_df.columns else 0
            has_time_count = final_df['é‡‡æ‘˜æ—¶é—´'].notna().sum() if 'é‡‡æ‘˜æ—¶é—´' in final_df.columns else 0
            summary_info.append({
                'task_name': task_name,
                'new_count': new_count,
                'total_count': len(final_df)
            })
            logging.info(f"â„¹ï¸ {sponge_type} æ–°å¢ {new_count} æ¡ï¼Œå½“å‰å…± {len(final_df)} æ¡ï¼ˆå…¶ä¸­{has_time_count}æ¡æœ‰é‡‡æ‘˜æ—¶é—´ï¼‰ã€‚")
        
        return {"data_frames": final_data_frames, "summary": summary_info}

    def _save_and_highlight(self, data_frames: Dict[str, pd.DataFrame]) -> None:
        """ä¿å­˜æ¸…å•åˆ°Excelå¹¶é«˜äº®æ–°æ¡ç›®"""
        if not data_frames:
            logging.info("æš‚æ— æ•°æ®éœ€è¦ä¿å­˜ã€‚")
            return
            
        try:
            # ä¿å­˜Excelæ–‡ä»¶
            with pd.ExcelWriter(self.filename, engine='openpyxl') as writer:
                for sheet_name, df in data_frames.items():
                    # éšè—is_newå­—æ®µï¼Œä¿ç•™é‡‡æ‘˜æ—¶é—´ç­‰å±•ç¤ºå­—æ®µ
                    display_df = df.drop(columns=['is_new'], errors='ignore')
                    
                    # è°ƒè¯•ï¼šç¡®è®¤é‡‡æ‘˜æ—¶é—´åˆ—æ˜¯å¦å­˜åœ¨
                    sponge_type = self._get_sponge_by_sheet(sheet_name)
                    if 'é‡‡æ‘˜æ—¶é—´' in display_df.columns:
                        has_time = display_df['é‡‡æ‘˜æ—¶é—´'].notna().sum()
                        logging.info(f"ğŸ“ å‡†å¤‡ä¿å­˜ {sponge_type}ï¼šåŒ…å«é‡‡æ‘˜æ—¶é—´åˆ—ï¼Œ{has_time}æ¡æœ‰æ—¶é—´è®°å½•")
                        # ç¡®ä¿é‡‡æ‘˜æ—¶é—´åˆ—åœ¨é å‰ä½ç½®ï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
                        cols = ['é‡‡æ‘˜æ—¶é—´'] + [col for col in display_df.columns if col != 'é‡‡æ‘˜æ—¶é—´']
                        display_df = display_df[cols]
                    else:
                        logging.warning(f"âš ï¸ {sponge_type} æ•°æ®ä¸­ç¼ºå°‘é‡‡æ‘˜æ—¶é—´åˆ—ï¼")
                    
                    display_df.to_excel(writer, sheet_name=sheet_name, index=False)
            
            # ä¸ºæ–°æ¡ç›®æ·»åŠ é»„è‰²é«˜äº®
            workbook = openpyxl.load_workbook(self.filename)
            highlight_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
            
            for sheet_name, df in data_frames.items():
                if sheet_name in workbook.sheetnames and 'is_new' in df.columns:
                    worksheet = workbook[sheet_name]
                    sponge_type = self._get_sponge_by_sheet(sheet_name)
                    new_count = 0
                    for row_idx, is_new in enumerate(df['is_new'], start=2):  # è·³è¿‡è¡¨å¤´
                        if is_new:
                            new_count += 1
                            for col_idx in range(1, worksheet.max_column + 1):
                                worksheet.cell(row=row_idx, column=col_idx).fill = highlight_fill
                    if new_count > 0:
                        logging.info(f"âœ¨ {sponge_type} å·²æ ‡è®° {new_count} æ¡æ–°æ¡ç›®ã€‚")
            
            workbook.save(self.filename)
            logging.info(f"ğŸ’¾ æ¸…å•å·²ä¿å­˜ã€‚")
            
            # ä¿å­˜JSONç¼“å­˜
            self._save_json_cache(data_frames)
            
        except Exception as e:
            logging.error(f"âš ï¸ ä¿å­˜æ¸…å•æ—¶é‡åˆ°é—®é¢˜: {e}")
            # å³ä½¿Excelä¿å­˜å¤±è´¥ï¼Œä»å°è¯•ä¿å­˜ç¼“å­˜
            try:
                self._save_json_cache(data_frames)
            except Exception as cache_error:
                logging.error(f"âš ï¸ å­˜æ¡£æ—¶é‡åˆ°é—®é¢˜: {cache_error}")

    @staticmethod
    def _send_notification(summary: List[Dict[str, Any]]) -> None:
        """å‘é€éšæ™¦åŒ–æ¡Œé¢é€šçŸ¥ï¼ˆæ ¸å¿ƒï¼šå…¨ç¨‹ç”¨ä¸ç“œè¯­å¢ƒï¼‰"""
        # æ‹›è˜ç±»å‹â†’ä¸ç“œä»£ç§°æ˜ å°„
        job_to_sponge = {'å®ä¹ æ‹›è˜': 'æ–°ä¸ç“œ', 'æ ¡å›­æ‹›è˜': 'ç”Ÿä¸ç“œ', 'ç¤¾ä¼šæ‹›è˜': 'ç†Ÿä¸ç“œ'}
        # ç»Ÿè®¡å„ç±»å‹ä¸ç“œæ•°æ®
        type_stats = {}
        total_new = 0
        total_count = 0
        for info in summary:
            job_type = info['task_name']
            sponge_type = job_to_sponge.get(job_type, job_type)
            new_cnt = info.get('new_count', 0)
            total_cnt = info.get('total_count', 0)
            type_stats[sponge_type] = {'new': new_cnt, 'total': total_cnt}
            total_new += new_cnt
            total_count += total_cnt
        current_time = datetime.now().strftime("%H:%M")

        # æœ‰æ–°å¢æ—¶çš„é€šçŸ¥å†…å®¹
        if total_new > 0:
            title = "ğŸ‰ é‡‡åˆ°æ–°ä¸ç“œå•¦!"
            details = "\\n".join(
                f"   â€¢ {sponge_type}: å¤šé‡‡{stats['new']}æ ¹ï¼Œå…±{stats['total']}æ ¹"
                for sponge_type, stats in type_stats.items() if stats['new'] > 0
            )
            message = f"è¿™æ¬¡é‡‡åˆ° {total_new} æ ¹æ–°ä¸ç“œï¼\\næ€»å…±å­˜äº† {total_count} æ ¹ä¸ç“œ\\næ—¶é—´: {current_time}\\n\\nåˆ†ç±»æƒ…å†µ:\\n{details}"
            buttons = '{"ç¨åæ•´ç†", "ç°åœ¨çœ‹çœ‹"}'
            default_button = '"ç°åœ¨çœ‹çœ‹"'
            action_script = f'do shell script "open \\"{OUTPUT_FILENAME}\\""'
        # æ— æ–°å¢æ—¶çš„é€šçŸ¥å†…å®¹
        else:
            title = "âœ… ä¸ç“œæ¸…ç‚¹å®Œå•¦"
            details = "\\n".join(
                f"   â€¢ {sponge_type}: è¿˜æ˜¯{stats['total']}æ ¹"
                for sponge_type, stats in type_stats.items()
            )
            message = f"è¿™æ¬¡æ²¡é‡‡åˆ°æ–°ä¸ç“œï½\\næ€»å…±è¿˜å­˜ç€ {total_count} æ ¹ä¸ç“œ\\næ—¶é—´: {current_time}\\n\\nå­˜é‡æƒ…å†µ:\\n{details}"
            buttons = '{"çŸ¥é“å•¦"}'
            default_button = '"çŸ¥é“å•¦"'
            action_script = ""

        # macOS é€šçŸ¥ï¼ˆAppleScriptï¼‰
        if sys.platform == "darwin":
            script = f'''
            try
                set response to display dialog "{message}" with title "{title}" buttons {buttons} default button {default_button} with icon note
                if button returned of response is "ç°åœ¨çœ‹çœ‹" then
                    {action_script}
                end if
            on error errMsg number errNum
                -- é™é»˜å¤„ç†ç”¨æˆ·å–æ¶ˆç­‰å¼‚å¸¸
            end try
            '''
            try:
                result = subprocess.run(
                    ['osascript', '-e', script], 
                    capture_output=True, text=True, timeout=120, check=False
                )
                if result.returncode != 0:
                    logging.error("âš ï¸ ä¸ç“œé€šçŸ¥å‘é€å¤±è´¥! è„šæœ¬æ‰§è¡Œå‡ºé”™ã€‚")
                    logging.error(f"   - è¿”å›ç : {result.returncode}")
                    logging.error(f"   - é”™è¯¯è¾“å‡º: {result.stderr.strip()}")
                else:
                    logging.info("ğŸ”” ä¸ç“œé€šçŸ¥å·²å‘é€ã€‚")
            except subprocess.TimeoutExpired:
                logging.warning("âš ï¸ ä¸ç“œé€šçŸ¥è¶…æ—¶ï¼Œç”¨æˆ·å¯èƒ½æœªåŠæ—¶å“åº”ã€‚")
            except Exception as e:
                logging.error(f"âš ï¸ å‘é€ä¸ç“œé€šçŸ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # émacOS æ—¥å¿—é€šçŸ¥
        else:
            logging.info("--- ä¸ç“œæ¸…ç‚¹é€šçŸ¥ ---")
            logging.info(f"æ ‡é¢˜: {title}")
            logging.info(message.replace("\\n", "\n"))
            logging.info("---------------------")

    async def run_async(self, silent_mode: bool = False):
        """æ‰§è¡Œå®Œæ•´çš„æ¸…å•æ•´ç†æµç¨‹ï¼ˆå¼‚æ­¥ï¼‰"""
        start_time = datetime.now()
        logging.info(f"--- å¼€å§‹æ•´ç† {start_time.strftime('%Y-%m-%d %H:%M:%S')} ---")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        DOCUMENTS_PATH.mkdir(exist_ok=True)
        
        # åŠ è½½å†å²æ¸…å•
        existing_hashes, existing_dataframes = self._load_existing_hashes()
        
        # å¼‚æ­¥æŠ“å–å„ç±»å‹ä¸ç“œæ¸…å•
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=self.headless)
            tasks_to_run = [self._run_single_task_async(task, browser) for task in self.tasks]
            await asyncio.gather(*tasks_to_run)
            await browser.close()

        # å¤„ç†æ•´ç†ç»“æœ
        results = self._process_results(existing_hashes, existing_dataframes)
        data_frames = results["data_frames"]
        summary = results["summary"]
        
        # ä¿å­˜å¹¶é«˜äº®æ–°æ¡ç›®
        self._save_and_highlight(data_frames)
        
        # è¾“å‡ºæ•´ç†ç»“æœæ—¥å¿—
        total_new = sum(info.get('new_count', 0) for info in summary)
        if not silent_mode or total_new > 0:
            logging.info("--- æ•´ç†ç»“æœ ---")
            for info in summary:
                sponge_type = self.job_to_sponge.get(info['task_name'], info['task_name'])
                logging.info(f"  - {sponge_type}: æ–°å¢{info.get('new_count', 0)}æ¡ï¼Œå…±{info.get('total_count', 0)}æ¡")
            logging.info(f"æ€»è®¡æ–°å¢: {total_new} æ¡")
        
        # å‘é€é€šçŸ¥
        self._send_notification(summary)
        
        # æµç¨‹ç»“æŸ
        end_time = datetime.now()
        logging.info(f"--- æ•´ç†å®Œæˆ, è€—æ—¶: {(end_time - start_time).total_seconds():.2f} ç§’ ---")

# --- 3. ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    try:
        # æ”¯æŒ--autoå‚æ•°å¼€å¯é™é»˜æ¨¡å¼ï¼ˆæ— æ–°å¢æ—¶ä¸è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼‰
        is_silent = "--auto" in sys.argv
        monitor = JobMonitor(tasks=TASK_CONFIGS, filename=OUTPUT_FILENAME, headless=True)
        asyncio.run(monitor.run_async(silent_mode=is_silent))
        
    except KeyboardInterrupt:
        logging.info("\nâš ï¸ æ•´ç†å·²ä¸­æ–­ã€‚")
        sys.exit(0)
    except Exception as e:
        if "Executable doesn't exist" in str(e):
            logging.critical("âŒ ä¾èµ–æœªå®‰è£…! è¯·å…ˆè¿è¡Œ 'playwright install' å‘½ä»¤ã€‚")
        else:
            logging.critical(f"\nâŒ æ•´ç†é‡åˆ°ä¸¥é‡é—®é¢˜: {e}", exc_info=True)
        sys.exit(1)
